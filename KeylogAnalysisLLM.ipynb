{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "import json\n",
    "import time\n",
    "import sqlite3\n",
    "load_dotenv()\n",
    "from keylog_analysis_helper import *\n",
    "from gemini_prompts import *\n",
    "import difflib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes\n",
    "- Code reconstruction may be incorrect, double check again\n",
    "- Measures of progress:\n",
    "    - Whether code compiles,\n",
    "    - Ask llm if it progressed?\n",
    "    - '# characters written\n",
    "        - Those that are code and not (ask llm)\n",
    "\n",
    "\n",
    "- does this portion make sense\n",
    "- does this portion make sense to the solution \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "\n",
    "# Create the model\n",
    "generation_config = {\n",
    "  \"temperature\": 0,\n",
    "  \"top_p\": 0.95,\n",
    "  \"top_k\": 1,\n",
    "  \"max_output_tokens\": 8192,\n",
    "  \"response_mime_type\": \"text/plain\",\n",
    "}\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "  model_name=\"gemini-1.5-pro-002\",\n",
    "  generation_config=generation_config,\n",
    "  system_instruction= system_instructions,\n",
    ")\n",
    "\n",
    "# chat_session = model.start_chat(\n",
    "#   history=[\n",
    "#   ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = model.generate_content(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How can I help you today?\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.parts[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.usage_metadata.total_token_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save dataframes with diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_context(file1_name, file1_prev, file1_current, file1_diff, file2_name, file2_prev, file2_current, file2_diff, instructions):\n",
    "\n",
    "    context = f\"\"\"\n",
    "        You will help me determine whether a student has made progress towards completing their coding assignment.\n",
    "        I will provide you with the code from the previous snapshot and the current snapshot. I will also provide you with the diff between the two files.\n",
    "        \n",
    "        Here is the previous snapshot for the file named {file1_name} : {file1_prev}\n",
    "        Here is the current snapshot for the file named {file1_name} : {file1_current}\n",
    "        Here is the diff between the two files:\n",
    "        {file1_diff}\n",
    "\n",
    "        \n",
    "        Here is the previous snapshot for the file named {file2_name} : {file2_prev}\n",
    "        Here is the current snapshot for the file named {file2_name} : {file2_current}\n",
    "        Here is the diff between the two files:\n",
    "        {file2_diff}\n",
    "\n",
    "        The instructions for this assignment were as follows {instructions}\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_prompt(file1_name, file1_prev, file1_current, file1_diff, file2_name, file2_prev, file2_current, file2_diff, instructions):\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You will help me determine whether a student has made progress towards completing their coding assignment.\n",
    "    I will provide you with the code from the previous snapshot and the current snapshot. I will also provide you with the diff between the two files.\n",
    "    \n",
    "    Here is the previous snapshot for the file named {file1_name} : {file1_prev}\n",
    "    Here is the current snapshot for the file named {file1_name} : {file1_current}\n",
    "    Here is the diff between the two files:\n",
    "    {file1_diff}\n",
    "\n",
    "    \n",
    "    Here is the previous snapshot for the file named {file2_name} : {file2_prev}\n",
    "    Here is the current snapshot for the file named {file2_name} : {file2_current}\n",
    "    Here is the diff between the two files:\n",
    "    {file2_diff}\n",
    "\n",
    "    The instructions for this assignment were as follows {instructions}\n",
    "    \"\"\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "\n",
    "    # The autograder code that is used to score the student is as follows {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions_a3 = read_file_as_string(\"archive/CompleteAssignments/a3_P0_c0/Project3.txt\")\n",
    "instructions_a4 = read_file_as_string(\"archive/CompleteAssignments/a4_P0_c0/Project4.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "differ = difflib.Differ()\n",
    "\n",
    "def make_llm_comparisons(a3=True):\n",
    "\n",
    "    # Read df based on assignment type\n",
    "    if a3:\n",
    "        df = pd.read_csv(\"KeylogDatasets/a3.csv\")\n",
    "        # df.drop(columns=[\"Unnamed: 0\", \"autograder_error\",\"bustersAgents.py_length\",\"inference.py_length\"], inplace=True)\n",
    "    else:\n",
    "        df = pd.read_csv(\"KeylogDatasets/a4.csv\")\n",
    "        # df.drop(columns=[\"Unnamed: 0\",\"autograder_error\",\"NeuralNet.py_length\",\"NeuralNetUtil.py_length\"], inplace=True)\n",
    "   \n",
    "    df[\"Prediction\"] = None\n",
    "    df['Diff'] = None\n",
    "    p_list = p_list_a3_log_analysis if a3 else p_list_a4_log_analysis\n",
    "    assignment = 'a3' if a3 else 'a4'\n",
    "    snapshots_dir = snapshots_dir_a3 if a3 else snapshots_dir_a4\n",
    "\n",
    "    total_token = 0\n",
    "    for p in p_list[:1]:\n",
    "        print(\"--Now working on participant\", p)\n",
    "        df_temp = df[df[\"participant\"] == p]\n",
    "\n",
    "        for idx, row in df_temp.iterrows(): # start from snapshot 1\n",
    "            if row['snapshot'] == 0:\n",
    "                continue\n",
    "            # Read files\n",
    "            current_snapshot = row['snapshot']\n",
    "            prev_snapshot = row['snapshot'] - 1\n",
    "            print(\"Now comparing snapshots\", prev_snapshot, current_snapshot)\n",
    "            \n",
    "            if a3:\n",
    "                prev_snapshot_dir = os.path.join(snapshots_dir, f'{assignment}_{p}_c{current_snapshot}', \"tracking\")\n",
    "                current_snapshot_dir = os.path.join(snapshots_dir, f'{assignment}_{p}_c{prev_snapshot}', \"tracking\")\n",
    "                file1_prev = read_file_as_string(os.path.join(prev_snapshot_dir, \"bustersAgents.py\"))\n",
    "                file1_current = read_file_as_string(os.path.join(current_snapshot_dir, \"bustersAgents.py\"))\n",
    "                file2_prev = read_file_as_string(os.path.join(prev_snapshot_dir, \"inference.py\"))\n",
    "                file2_current = read_file_as_string(os.path.join(current_snapshot_dir, \"inference.py\"))\n",
    "            else:\n",
    "                prev_snapshot_dir = os.path.join(snapshots_dir, f'{assignment}_{p}_c{prev_snapshot}')\n",
    "                current_snapshot_dir = os.path.join(snapshots_dir, f'{assignment}_{p}_c{current_snapshot}')\n",
    "                file1_prev = read_file_as_string(os.path.join(prev_snapshot_dir, \"NeuralNet.py\"))\n",
    "                file1_current = read_file_as_string(os.path.join(current_snapshot_dir, \"NeuralNet.py\"))\n",
    "                file2_prev = read_file_as_string(os.path.join(prev_snapshot_dir, \"NeuralNetUtil.py\"))\n",
    "                file2_current = read_file_as_string(os.path.join(current_snapshot_dir, \"NeuralNetUtil.py\"))\n",
    "\n",
    "            # Run diff\n",
    "            file1_diff = differ.compare(file1_prev, file1_current)\n",
    "            file2_diff = differ.compare(file2_prev, file2_current)\n",
    "            \n",
    "            # Read autograder code contents\n",
    "            if a3:\n",
    "                autograder_code = None\n",
    "                assignment_instructions = None\n",
    "                instructions = instructions_a3\n",
    "                file1_name = \"bustersAgents.py\"\n",
    "                file2_name = \"inference.py\"\n",
    "            else:\n",
    "                autograder_code = None\n",
    "                assignment_instructions = None\n",
    "                instructions = instructions_a4\n",
    "                file1_name = \"NeuralNet.py\"\n",
    "                file2_name = \"NeuralNetUtil.py\"\n",
    "\n",
    "            # Ask the llm\n",
    "            prompt = form_prompt(file1_name, file1_prev, file1_current, file1_diff, file2_name, file2_prev, file2_current, file2_diff, instructions)\n",
    "            print (f\"Prompt of length {len(prompt)}\")\n",
    "            # return prompt \n",
    "            \n",
    "\n",
    "            try:\n",
    "                response = model.generate_content(prompt)\n",
    "                total_token += response.usage_metadata.total_token_count\n",
    "                response = response.parts[0].text\n",
    "            except Exception as e:\n",
    "                print(\"Gemini Exception: \", e)\n",
    "                response = None\n",
    "    \n",
    "            print(\"Response is \", response)\n",
    "            df.at[idx, \"Prediction\"] = response\n",
    "            # df.to_csv(\"KeylogDatasets/a3_llm.csv\", index=False) if a3 else df.to_csv(\"KeylogDatasets/a4.csv\", index=False)\n",
    "            time.sleep(1)\n",
    "            print(\"Total tokens used: \", total_token)\n",
    "            \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Now working on participant P3\n",
      "Now comparing snapshots 0 1\n",
      "Prompt of length 74808\n",
      "Response is  1\n",
      "\n",
      "Total tokens used:  17692\n",
      "Now comparing snapshots 1 2\n",
      "Prompt of length 75191\n",
      "Response is  1\n",
      "\n",
      "Total tokens used:  35475\n",
      "Now comparing snapshots 2 3\n",
      "Prompt of length 75616\n",
      "Response is  1\n",
      "\n",
      "Total tokens used:  53355\n",
      "Now comparing snapshots 3 4\n",
      "Prompt of length 75796\n",
      "Response is  1\n",
      "\n",
      "Total tokens used:  71273\n",
      "Now comparing snapshots 4 5\n",
      "Prompt of length 75798\n",
      "Response is  1\n",
      "\n",
      "Total tokens used:  89189\n",
      "Now comparing snapshots 5 6\n",
      "Prompt of length 75819\n",
      "Response is  1\n",
      "\n",
      "Total tokens used:  107108\n",
      "Now comparing snapshots 6 7\n",
      "Prompt of length 75989\n",
      "Response is  1\n",
      "\n",
      "Total tokens used:  125064\n",
      "Now comparing snapshots 7 8\n",
      "Prompt of length 76162\n",
      "Response is  1\n",
      "\n",
      "Total tokens used:  143052\n",
      "Now comparing snapshots 8 9\n",
      "Prompt of length 76264\n",
      "Response is  1\n",
      "\n",
      "Total tokens used:  161066\n",
      "Now comparing snapshots 9 10\n",
      "Prompt of length 76499\n",
      "Response is  1\n",
      "\n",
      "Total tokens used:  179130\n",
      "Now comparing snapshots 10 11\n",
      "Prompt of length 76749\n",
      "Response is  1\n",
      "\n",
      "Total tokens used:  197241\n",
      "Now comparing snapshots 11 12\n",
      "Prompt of length 76787\n",
      "Response is  1\n",
      "\n",
      "Total tokens used:  215362\n",
      "Now comparing snapshots 12 13\n",
      "Prompt of length 76805\n",
      "Response is  1\n",
      "\n",
      "Total tokens used:  233490\n"
     ]
    }
   ],
   "source": [
    "df_a3 = make_llm_comparisons(a3=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
