{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "import json\n",
    "import time\n",
    "import sqlite3\n",
    "load_dotenv()\n",
    "from keylog_analysis_helper import *\n",
    "import difflib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes\n",
    "- Code reconstruction may be incorrect, double check again\n",
    "- Measures of progress:\n",
    "    - Whether code compiles,\n",
    "    - Ask llm if it progressed?\n",
    "    - '# characters written\n",
    "        - Those that are code and not (ask llm)\n",
    "\n",
    "\n",
    "- does this portion make sense\n",
    "- does this portion make sense to the solution \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "\n",
    "# Create the model\n",
    "generation_config = {\n",
    "  \"temperature\": 1,\n",
    "  \"top_p\": 0.95,\n",
    "  \"top_k\": 40,\n",
    "  \"max_output_tokens\": 8192,\n",
    "  \"response_mime_type\": \"text/plain\",\n",
    "}\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "  model_name=\"models/gemini-1.5-pro-002\",\n",
    "  # generation_config=generation_config,\n",
    ")\n",
    "\n",
    "# chat_session = model.start_chat(\n",
    "#   history=[\n",
    "#   ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save dataframes with diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_prompt():\n",
    "\n",
    "    \"\"\"\n",
    "    You will help me determine whether a student has made progress towards completing their coding assignment.\n",
    "    I will provide you with the code from the previous snapshot and the current snapshot. I will also provide you with the diff between the two files.\n",
    "    \n",
    "    Here is the previous snapshot for the file named {}\n",
    "    Here is the current snapshot for the file named {}\n",
    "    Here is the diff between the two files:\n",
    "    {}\n",
    "    \n",
    "    Here is the previous snapshot for the file named {}\n",
    "    Here is the current snapshot for the file named {}\n",
    "    Here is the diff between the two files:\n",
    "    {}\n",
    "\n",
    "    The instructions for the assignment are as follows {}\n",
    "\n",
    "    The autograder code that is used to score the student is as follows {}\n",
    "\n",
    "    If you believe the student has progressed towards passing more tests in the autograder than simply answer with \"1\".\n",
    "    If you believe it is the same or the student has regressed then answer with \"0\".\n",
    "    If you believe it has regressed then answer with \"-1\".\n",
    "\n",
    "    Simply answer with one of these three options. Do not give me an explanation.\n",
    "\n",
    "\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "differ = difflib.Differ()\n",
    "\n",
    "def make_llm_comparisons(a3=True):\n",
    "\n",
    "    # Read df based on assignment type\n",
    "    if a3:\n",
    "        df = pd.read_csv(\"KeylogDatasets/a3.csv\")\n",
    "        df.drop(columns=[\"Unnamed: 0\", \"autograder_error\",\"bustersAgents.py_length\",\"inference.py_length\"], inplace=True)\n",
    "    else:\n",
    "        df = pd.read_csv(\"KeylogDatasets/a4.csv\")\n",
    "        df.drop(columns=[\"Unnamed: 0\",\"autograder_error\",\"NeuralNet.py_length\",\"NeuralNetUtil.py_length\"], inplace=True)\n",
    "   \n",
    "    df[\"Prediction\"] = None\n",
    "    df['Diff'] = None\n",
    "    p_list = p_list_a3_log_analysis if a3 else p_list_a4_log_analysis\n",
    "    assignment = 'a3' if a3 else 'a4'\n",
    "    snapshots_dir = snapshots_dir_a3 if a3 else snapshots_dir_a4\n",
    "\n",
    "    for p in p_list:\n",
    "        df_temp = df[df[\"participant\"] == p]\n",
    "\n",
    "        # Create chat session\n",
    "        chat_session = model.start_chat(\n",
    "            history=[\n",
    "                row[\"text\"]\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        for idx, row in df_temp.iterrows()[1:]:\n",
    "            print(row[\"participant\"], row['snapshot'])\n",
    "\n",
    "            # read snapshot file path\n",
    "            # Read files\n",
    "            prev_snapshot_dir = os.path.join(snapshots_dir, f'{assignment}_{p}_c{row['snapshot']-1}')\n",
    "            current_snapshot_dir = os.path.join(snapshots_dir, f'{assignment}_{p}_c{row['snapshot']}')\n",
    "            \n",
    "            if a3:\n",
    "                prev_file_1 = read_file_as_string(os.path.join(prev_snapshot_dir, \"bustersAgents.py\"))\n",
    "                current_file_1 = read_file_as_string(os.path.join(current_snapshot_dir, \"bustersAgents.py\"))\n",
    "                prev_file_2 = read_file_as_string(os.path.join(prev_snapshot_dir, \"inference.py\"))\n",
    "                current_file_2 = read_file_as_string(os.path.join(current_snapshot_dir, \"inference.py\"))\n",
    "            else:\n",
    "                prev_file_1 = read_file_as_string(os.path.join(prev_snapshot_dir, \"NeuralNet.py\"))\n",
    "                current_file_1 = read_file_as_string(os.path.join(current_snapshot_dir, \"NeuralNet.py\"))\n",
    "                prev_file_2 = read_file_as_string(os.path.join(prev_snapshot_dir, \"NeuralNetUtil.py\"))\n",
    "                current_file_2 = read_file_as_string(os.path.join(current_snapshot_dir, \"NeuralNetUtil.py\"))\n",
    "\n",
    "            # Run diff\n",
    "            diff_1 = differ.compare(prev_file_1, current_file_1)\n",
    "            diff_2 = differ.compare(prev_file_2, current_file_2)\n",
    "            \n",
    "\n",
    "            # Read autograder code contents\n",
    "            if a3:\n",
    "                autograder_code = None\n",
    "                assignment_instructions = None\n",
    "            else:\n",
    "                autograder_code = None\n",
    "                assignment_instructions = None\n",
    "\n",
    "            # Ask the llm\n",
    "            \n",
    "            prompt = form_prompt()\n",
    "\n",
    "\n",
    "            response = chat_session.send_message()\n",
    "            print(response)\n",
    "            df.at[idx, \"llm_response\"] = response\n",
    "            df.to_csv(\"KeylogDatasets/a3_llm.csv\", index=False) if a3 else df.to_csv(\"KeylogDatasets/a4.csv\", index=False)\n",
    "            time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
